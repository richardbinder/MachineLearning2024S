{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets and basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv('./BreastCancer/dataset/breast-cancer-diagnostic.shuf.lrn.csv')\n",
    "test_data = pd.read_csv('./BreastCancer/dataset/breast-cancer-diagnostic.shuf.tes.csv')\n",
    "test_data_class = pd.read_csv('./BreastCancer/dataset/breast-cancer-diagnostic.shuf.sol.ex.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is: 19\n",
      "Accuracy: 0.9894366197183099\n",
      "F1 Score: 0.9946902654867257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Splitting the dataset into attributes (X) and classes (Y)\n",
    "X_dataset = dataset.drop('class', axis=1)\n",
    "Y_classes = dataset['class'].astype(int)\n",
    "\n",
    "# Defining variables for valiation\n",
    "X_testing = test_data\n",
    "Y_validation = test_data_class['class'].astype(int)\n",
    "\n",
    "\n",
    "# Preprocessing the data by scaling it\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_dataset)\n",
    "X_validation = scaler.transform(X_testing)\n",
    "\n",
    "# Finding the best K value\n",
    "# Creating a list of K values\n",
    "k_values = list(range(1, 20))\n",
    "\n",
    "# Creating a list of cross validation scores\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Creating the KNN model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "    \n",
    "    # Doing cross validation\n",
    "    scores = cross_val_score(knn_model, X_dataset, Y_classes, cv=10, scoring='accuracy')\n",
    "    \n",
    "    # Saving the mean of the scores\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "# Finding the optimal K value by finding the one with the highest accuracy\n",
    "optimal_k = k_values[np.argmax(k_values)]\n",
    "print(f\"The optimal number of neighbors is: {optimal_k}\")\n",
    "\n",
    "\n",
    "# Creating the KNN model\n",
    "\"\"\"\n",
    "n_neighbors: Number of neighbors to use, in this case we are using the optimal_k\n",
    "weights: Weight function used in prediction, in this case we are using uniform.\n",
    "\"\"\"\n",
    "knn_model = KNeighborsClassifier(n_neighbors=optimal_k, weights='uniform')\n",
    "# Training the model\n",
    "knn_model.fit(X_train, Y_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(X_testing)\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(Y_validation, Y_predicted)}')\n",
    "\n",
    "# Calculating the F1 Score of the model (This is the one we are interested in for kaggle)\n",
    "f1 = f1_score(Y_validation, Y_predicted, average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Splitting the dataset into attributes (X) and classes (Y)\n",
    "X_dataset = dataset.drop('class', axis=1)\n",
    "Y_classes = dataset['class'].astype(int)\n",
    "\n",
    "# Defining variables for valiation\n",
    "X_testing = test_data\n",
    "Y_validation = test_data_class['class'].astype(int)\n",
    "\n",
    "\n",
    "# Preprocessing the data by scaling it\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_dataset)\n",
    "X_validation = scaler.transform(X_testing)\n",
    "\n",
    "\n",
    "# Initializing the MLP model\n",
    "\"\"\"\n",
    "- hidden_layer_sizes is the number of neurons in each layer, each number represents the number of neurons in a layer\n",
    "- max_iter is the number of iterations the model will do\n",
    "- activation is the activation function, in this case we are using the ReLU function (rectified linear unit)\n",
    "- solver is the optimization algorithm, in this case we are using the Adam optimizer\n",
    "- random_state is the seed for the random number generator, so the results are reproducible\n",
    "\"\"\"\n",
    "# While playing with the hidden layers I found that 5 neurons in the first layer and 3 in the second layer gave the best results for breast cancer dataset for this random seed 42\n",
    "# But this is not always the case, if we change the seed we usually get a result of around 0.70\n",
    "neural_network_model = MLPClassifier(hidden_layer_sizes=(5,3), max_iter=100, activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "# Training the model\n",
    "neural_network_model.fit(X_train, Y_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = neural_network_model.predict(X_validation)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(Y_validation, Y_predicted)}\")\n",
    "print(f\"F1 Score: {f1_score(Y_validation, Y_predicted, average='weighted')}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6690140845070423\n",
      "F1 Score: 0.8016877637130801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Splitting the dataset into attributes (X) and classes (Y)\n",
    "X_dataset = dataset.drop('class', axis=1)\n",
    "Y_classes = dataset['class'].astype(int)\n",
    "\n",
    "# Defining variables for valiation\n",
    "X_testing = test_data\n",
    "Y_validation = test_data_class['class'].astype(int)\n",
    "\n",
    "\n",
    "# Preprocessing the data by scaling it\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_dataset)\n",
    "X_validation = scaler.transform(X_testing)\n",
    "\n",
    "# Initializing the Random Forest model\n",
    "random_forest_model = RandomForestClassifier(\n",
    "                        max_leaf_nodes=2,\n",
    "                        max_features=5,\n",
    "                        max_depth=6,\n",
    "                        random_state=42\n",
    "                    )\n",
    "\n",
    "# Fit the model to the data\n",
    "random_forest_model.fit(X_train, Y_classes)\n",
    "\n",
    "# Predict the classes of the validation set\n",
    "Y_predicted = random_forest_model.predict(X_validation)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(Y_validation, Y_predicted)}\")\n",
    "print(f\"F1 Score: {f1_score(Y_validation, Y_predicted, average='weighted')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
