{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saito/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saito/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn\n",
    "!pip3 install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "# Note, for now MinMaxScaler > normalize > StandardScaler > MaxAbsScaler (but not by much)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, normalize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset and test the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./dataset/breast-cancer-diagnostic.shuf.lrn.csv')\n",
    "test_data = pd.read_csv('./dataset/breast-cancer-diagnostic.shuf.tes.csv')\n",
    "test_data_class = pd.read_csv('./dataset/breast-cancer-diagnostic.shuf.sol.ex.csv')\n",
    "\n",
    "# Delete whitespace in column names\n",
    "dataset.columns = dataset.columns.str.strip()\n",
    "test_data.columns = test_data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values were loaded correctly\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for missing values\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = {\n",
    "    'class': 'Class (Malignant/Benign)',\n",
    "    'concavePointsWorst': 'Concave Points Worst',\n",
    "    'concavePointsMean': 'Concave Points Mean',\n",
    "    'radiusWorst': 'Radius Worst',\n",
    "    'perimeterWorst': 'Perimeter Worst',\n",
    "    'areaWorst': 'Area Worst',\n",
    "    'areaStdErr': 'Area StdErr',\n",
    "    'concavityWorst': 'Concavity Worst',\n",
    "    'concavityMean': 'Concavity Mean',\n",
    "    'perimeterMean': 'Perimeter Mean',\n",
    "    'areaMean': 'Area Mean',\n",
    "    'radiusMean': 'Radius Mean',\n",
    "    'smoothnessWorst': 'Smoothness Worst',\n",
    "    'perimeterStdErr': 'Perimeter StdErr',\n",
    "    'textureWorst': 'Texture Worst',\n",
    "    'symmetryWorst': 'Symmetry Worst',\n",
    "    'radiusStdErr': 'Radius StdErr',\n",
    "    'compactnessWorst': 'Compactness Worst',\n",
    "    'textureMean': 'Texture Mean',\n",
    "    'compactnessMean': 'Compactness Mean',\n",
    "    'smoothnessMean': 'Smoothness Mean',\n",
    "    'concavePointsStdErr': 'Concave Points StdErr',\n",
    "    'concavityStdErr': 'Concavity StdErr',\n",
    "    'fractalDimensionWorst': 'Fractal Dimension Worst',\n",
    "    'fractalDimensionMean': 'Fractal Dimension Mean',\n",
    "    'textureStdErr': 'Texture StdErr',\n",
    "    'symmetryStdErr': 'Symmetry StdErr',\n",
    "    'compactnessStdErr': 'Compactness StdErr',\n",
    "    'smoothnessStdErr': 'Smoothness StdErr',\n",
    "    'fractalDimensionStdErr': 'Fractal Dimension StdErr',\n",
    "    'symmetryMean': 'Symmetry Mean'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot_list = list(columns_to_plot.keys())\n",
    "columns_to_plot_1 = columns_to_plot_list[:columns_to_plot_list.index('radiusMean')]\n",
    "columns_to_plot_2 = columns_to_plot_list[columns_to_plot_list.index('radiusMean'):]\n",
    "\n",
    "def plot_columns(columns_to_plot, filename):\n",
    "    charts_per_row = 2\n",
    "    total_columns = len(columns_to_plot)\n",
    "    rows = math.ceil(total_columns / charts_per_row)\n",
    "\n",
    "    fig, axs = plt.subplots(rows * 2, charts_per_row, figsize=(20, rows * 10))  # Multiplicamos las filas por 2 para tener espacio para ambos gr√°ficos\n",
    "\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, column in enumerate(columns_to_plot):\n",
    "        if column != 'class':\n",
    "            unique_values = dataset[column].dropna().unique()\n",
    "            unique_count = len(unique_values)\n",
    "            bis = 15\n",
    "\n",
    "            if unique_count <= 4:\n",
    "                bis = unique_count\n",
    "                axs[i*2].set_xticks(unique_values)\n",
    "\n",
    "            sns.histplot(dataset[column], kde=True, ax=axs[i*2], color='skyblue', edgecolor='black', bins=bis)\n",
    "            max_count = max(np.histogram(dataset[column].dropna(), bins=bis)[0])\n",
    "            axs[i*2].set_ylim(top=max_count * 1.1) \n",
    "\n",
    "            axs[i*2].tick_params(axis='y', labelsize=16)\n",
    "            axs[i*2].tick_params(axis='x', labelsize=16)\n",
    "            axs[i*2].set_title(f'Distribution - {column}', fontsize=16)\n",
    "            axs[i*2].set_xlabel(column, fontsize=14)\n",
    "            axs[i*2].set_ylabel('Frequency', fontsize=14)\n",
    "\n",
    "            # Boxplot\n",
    "            sns.boxplot(x='class', y=column, data=dataset, ax=axs[i*2+1])\n",
    "            axs[i*2+1].tick_params(axis='y', labelsize=16)\n",
    "            axs[i*2+1].tick_params(axis='x', labelsize=16)\n",
    "            axs[i*2+1].set_title(f'Distribution - {column} by class', fontsize=16)\n",
    "            axs[i*2+1].set_xlabel('Class', fontsize=14)\n",
    "            axs[i*2+1].set_ylabel(column, fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(filename, dpi=fig.dpi)\n",
    "\n",
    "plot_columns(columns_to_plot_1, 'plot1.png')\n",
    "plot_columns(columns_to_plot_2, 'plot2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the ID column from the dataset and test_data\n",
    "dataset = dataset.drop('ID', axis=1)\n",
    "test_data = test_data.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the class to binary values\n",
    "dataset['class'] = dataset['class'].astype(int)\n",
    "test_data_class['class'] = test_data_class['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define valiables to split atttributes and class; X (attributes) and Y (class)\n",
    "X_attributes = dataset.drop('class', axis=1)\n",
    "Y_class = dataset['class']\n",
    "\n",
    "## Define the attributes and class for the test data; X (attributes) and Y (class)\n",
    "X_attributes_test = test_data\n",
    "Y_class_test = test_data_class['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MinMaxScaler, MaxAbsScaler and StandardScaler\n",
    "## Scale the data (standardize)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Fit the scaler to the dataset and transform the dataset\n",
    "X_attributes_scaled = dataset.copy()\n",
    "X_attributes_scaled = scaler.fit_transform(X_attributes)\n",
    "\n",
    "# Scale the test data, based on the scaler fitted to the training data\n",
    "X_attributes_test_scaled = test_data.copy()\n",
    "X_attributes_test_scaled = scaler.transform(X_attributes_test)\n",
    "\n",
    "\n",
    "# ## For normalization\n",
    "# # Fit the scaler to the dataset and transform the dataset\n",
    "# X_attributes_scaled = X_attributes_test.copy()\n",
    "# X_attributes_scaled = normalize(X_attributes, norm='l2')\n",
    "\n",
    "# # Scale the test data, based on the scaler fitted to the training data\n",
    "# X_attributes_test_scaled = X_attributes_test.copy()\n",
    "# X_attributes_test_scaled = normalize(X_attributes_test, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.35099588e-02 1.65003436e-02 8.84824879e-02 ... 1.32970511e-04\n",
      "  2.96909409e-04 7.70048295e-05]\n",
      " [1.63246545e-02 2.91327898e-02 1.10899010e-01 ... 3.68090940e-04\n",
      "  9.48888413e-04 2.47299933e-04]\n",
      " [1.02774462e-02 7.26202329e-03 6.79552246e-02 ... 9.45505814e-05\n",
      "  1.31293534e-04 4.16772807e-05]\n",
      " ...\n",
      " [1.89387678e-02 3.38575142e-02 1.23780928e-01 ... 1.41611957e-04\n",
      "  5.25282806e-04 1.35555124e-04]\n",
      " [1.54676943e-02 2.48575749e-02 9.92026662e-02 ... 7.04411773e-05\n",
      "  2.78395734e-04 7.99448747e-05]\n",
      " [1.53230940e-02 2.45069271e-02 9.84035814e-02 ... 1.35314321e-04\n",
      "  3.34276490e-04 1.02412895e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(X_attributes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        importance\n",
      "concavePointsWorst        0.192535\n",
      "concavePointsMean         0.117753\n",
      "concavityMean             0.099546\n",
      "areaWorst                 0.089077\n",
      "perimeterWorst            0.081338\n",
      "radiusWorst               0.052917\n",
      "concavityWorst            0.050328\n",
      "perimeterMean             0.048775\n",
      "areaStdErr                0.046046\n",
      "areaMean                  0.039142\n",
      "radiusMean                0.033618\n",
      "compactnessWorst          0.017061\n",
      "smoothnessWorst           0.015205\n",
      "perimeterStdErr           0.015106\n",
      "compactnessMean           0.015102\n",
      "textureWorst              0.014510\n",
      "radiusStdErr              0.014154\n",
      "textureMean               0.010744\n",
      "symmetryWorst             0.009702\n",
      "fractalDimensionWorst     0.006551\n",
      "concavityStdErr           0.004985\n",
      "compactnessStdErr         0.003936\n",
      "symmetryMean              0.003893\n",
      "symmetryStdErr            0.003036\n",
      "textureStdErr             0.002986\n",
      "smoothnessStdErr          0.002789\n",
      "fractalDimensionMean      0.002696\n",
      "smoothnessMean            0.002564\n",
      "fractalDimensionStdErr    0.002189\n",
      "concavePointsStdErr       0.001718\n",
      "['concavePointsWorst', 'concavePointsMean', 'concavityMean', 'areaWorst', 'perimeterWorst', 'radiusWorst', 'concavityWorst']\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest model to get which features are more important\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X_attributes, Y_class)\n",
    "\n",
    "# Get the most important features\n",
    "most_important_attributes = pd.DataFrame(\n",
    "                            model.feature_importances_,\n",
    "                            index = X_attributes.columns,\n",
    "                            columns=['importance']\n",
    "                        ).sort_values('importance', ascending=False)\n",
    "\n",
    "print(most_important_attributes)\n",
    "\n",
    "\n",
    "## Get a list of the most important features whose importance is greater than 0.05\n",
    "most_important_attributes_list = most_important_attributes[most_important_attributes['importance'] > 0.05].index.tolist()\n",
    "print(most_important_attributes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the less important features\n",
    "X_attributes_less_attributes = X_attributes.copy()\n",
    "X_attributes_less_attributes = X_attributes_less_attributes[most_important_attributes_list]\n",
    "\n",
    "X_attributes_test_less_attributes = X_attributes_test.copy()\n",
    "X_attributes_test_less_attributes = X_attributes_test_less_attributes[most_important_attributes_list]\n",
    "\n",
    "\n",
    "# Remove the less important features from the scaled data\n",
    "X_attributes_scaled_less_attributes = X_attributes_scaled.copy()\n",
    "X_attributes_test_scaled_less_attributes = X_attributes_test_scaled.copy()\n",
    "\n",
    "X_attributes_scaled_less_attributes = pd.DataFrame(X_attributes_scaled_less_attributes, columns=X_attributes.columns)\n",
    "X_attributes_test_scaled_less_attributes = pd.DataFrame(X_attributes_test_scaled_less_attributes, columns=X_attributes_test.columns)\n",
    "\n",
    "X_attributes_scaled_less_attributes = X_attributes_scaled_less_attributes[most_important_attributes_list]\n",
    "X_attributes_test_scaled_less_attributes = X_attributes_test_scaled_less_attributes[most_important_attributes_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary of the variables with the data\n",
    "# X_attributes - Attributes of the training data, without the class column\n",
    "# Y_class - Class of the training data\n",
    "\n",
    "# X_attributes_test - Attributes of the test data, without the class column\n",
    "# Y_class_test - Class of the test data\n",
    "\n",
    "# X_attributes_scaled - Attributes of the training data, without the class column, scaled\n",
    "# X_attributes_test_scaled - Attributes of the test data, without the class column, scaled\n",
    "\n",
    "# X_attributes_less_attributes - Attributes of the training data, without the class column, with only the most important features\n",
    "# X_attributes_test_less_attributes - Attributes of the test data, without the class column, with only the most important features\n",
    "\n",
    "# X_attributes_scaled_less_attributes - Attributes of the training data, without the class column, with only the most important features, scaled\n",
    "# X_attributes_test_scaled_less_attributes - Attributes of the test data, without the class column, with only the most important features, scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1\n",
    "- `Neighbors`: 5\n",
    "- `Weights`: uniform\n",
    "- `Data normalization`: No\n",
    "- `All features`: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6408450704225352\n",
      "F1 Score: 0.7811158798283262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78       284\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.64       284\n",
      "   macro avg       0.50      0.32      0.39       284\n",
      "weighted avg       1.00      0.64      0.78       284\n",
      "\n",
      "[[182 102]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "weights = 'uniform'\n",
    "\n",
    "training_attributes = X_attributes\n",
    "training_classes = Y_class\n",
    "\n",
    "validation_data = X_attributes_test\n",
    "validation_classes = Y_class_test\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "# Training the model\n",
    "knn_model.fit(training_attributes, training_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(validation_data)\n",
    "\n",
    "## Metrics\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(validation_classes, Y_predicted)}')\n",
    "\n",
    "print(f\"F1 Score: {f1_score(validation_classes, Y_predicted, average='weighted')}\")\n",
    "\n",
    "# Classification report, which includes precision, recall, f1-score and support\n",
    "print(classification_report(validation_classes, Y_predicted))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(validation_classes, Y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2\n",
    "- `Neighbors`: 5\n",
    "- `Weights`: uniform\n",
    "- `Data normalization`: Yes\n",
    "- `All features`: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.647887323943662\n",
      "F1 Score: 0.7863247863247863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.65      0.79       284\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.65       284\n",
      "   macro avg       0.50      0.32      0.39       284\n",
      "weighted avg       1.00      0.65      0.79       284\n",
      "\n",
      "[[184 100]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "weights = 'distance'\n",
    "\n",
    "training_attributes = X_attributes_scaled\n",
    "training_classes = Y_class\n",
    "\n",
    "validation_data = X_attributes_test_scaled\n",
    "validation_classes = Y_class_test\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "# Training the model\n",
    "knn_model.fit(training_attributes, training_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(validation_data)\n",
    "\n",
    "## Metrics\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(validation_classes, Y_predicted)}')\n",
    "\n",
    "print(f\"F1 Score: {f1_score(validation_classes, Y_predicted, average='weighted')}\")\n",
    "\n",
    "# Classification report, which includes precision, recall, f1-score and support\n",
    "print(classification_report(validation_classes, Y_predicted))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(validation_classes, Y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3\n",
    "- `Neighbors`: 5\n",
    "- `Weights`: uniform\n",
    "- `Data normalization`: No\n",
    "- `All features`: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6690140845070423\n",
      "F1 Score: 0.8016877637130801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80       284\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67       284\n",
      "   macro avg       0.50      0.33      0.40       284\n",
      "weighted avg       1.00      0.67      0.80       284\n",
      "\n",
      "[[190  94]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "weights = 'distance'\n",
    "\n",
    "training_attributes = X_attributes_less_attributes\n",
    "training_classes = Y_class\n",
    "\n",
    "validation_data = X_attributes_test_less_attributes\n",
    "validation_classes = Y_class_test\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "# Training the model\n",
    "knn_model.fit(training_attributes, training_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(validation_data)\n",
    "\n",
    "## Metrics\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(validation_classes, Y_predicted)}')\n",
    "\n",
    "print(f\"F1 Score: {f1_score(validation_classes, Y_predicted, average='weighted')}\")\n",
    "\n",
    "# Classification report, which includes precision, recall, f1-score and support\n",
    "print(classification_report(validation_classes, Y_predicted))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(validation_classes, Y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4\n",
    "- `Neighbors`: 5\n",
    "- `Weights`: uniform\n",
    "- `Data normalization`: Yes\n",
    "- `All features`: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6056338028169014\n",
      "F1 Score: 0.7543859649122807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.61      0.75       284\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.61       284\n",
      "   macro avg       0.50      0.30      0.38       284\n",
      "weighted avg       1.00      0.61      0.75       284\n",
      "\n",
      "[[172 112]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "weights = 'uniform'\n",
    "\n",
    "training_attributes = X_attributes_scaled_less_attributes\n",
    "training_classes = Y_class\n",
    "\n",
    "validation_data = X_attributes_test_scaled_less_attributes\n",
    "validation_classes = Y_class_test\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "# Training the model\n",
    "knn_model.fit(training_attributes, training_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(validation_data)\n",
    "\n",
    "## Metrics\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(validation_classes, Y_predicted)}')\n",
    "\n",
    "print(f\"F1 Score: {f1_score(validation_classes, Y_predicted, average='weighted')}\")\n",
    "\n",
    "# Classification report, which includes precision, recall, f1-score and support\n",
    "print(classification_report(validation_classes, Y_predicted))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(validation_classes, Y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- Finding the best K ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is: 19\n"
     ]
    }
   ],
   "source": [
    "# Finding the best K value\n",
    "# Creating a list of K values in the range of 1 to 20\n",
    "k_values = list(range(1, 20))\n",
    "\n",
    "# Creating a list of cross validation scores\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Creating the KNN model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "    \n",
    "    # Doing cross validation\n",
    "    scores = cross_val_score(knn_model, X_attributes, Y_class, cv=10, scoring='accuracy')\n",
    "    \n",
    "    # Saving the mean of the scores\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "# Finding the optimal K value by finding the one with the highest accuracy\n",
    "optimal_k = k_values[np.argmax(k_values)]\n",
    "print(f\"The optimal number of neighbors is: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5\n",
    "- `Neighbors`: 19\n",
    "- `Weights`: uniform\n",
    "- `Data normalization`: No\n",
    "- `All features`: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6654929577464789\n",
      "F1 Score: 0.7991543340380549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80       284\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67       284\n",
      "   macro avg       0.50      0.33      0.40       284\n",
      "weighted avg       1.00      0.67      0.80       284\n",
      "\n",
      "[[189  95]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "k = 19\n",
    "weights = 'uniform'\n",
    "\n",
    "training_attributes = X_attributes\n",
    "training_classes = Y_class\n",
    "\n",
    "validation_data = X_attributes_test\n",
    "validation_classes = Y_class_test\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "# Training the model\n",
    "knn_model.fit(training_attributes, training_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(validation_data)\n",
    "\n",
    "## Metrics\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(validation_classes, Y_predicted)}')\n",
    "\n",
    "print(f\"F1 Score: {f1_score(validation_classes, Y_predicted, average='weighted')}\")\n",
    "\n",
    "# Classification report, which includes precision, recall, f1-score and support\n",
    "print(classification_report(validation_classes, Y_predicted))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(validation_classes, Y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6\n",
    "- `Neighbors`: 19\n",
    "- `Weights`: uniform\n",
    "- `Data normalization`: Yes\n",
    "- `All features`: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6619718309859155\n",
      "F1 Score: 0.7966101694915254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.80       284\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.66       284\n",
      "   macro avg       0.50      0.33      0.40       284\n",
      "weighted avg       1.00      0.66      0.80       284\n",
      "\n",
      "[[188  96]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "k = 19\n",
    "weights = 'uniform'\n",
    "\n",
    "training_attributes = X_attributes_scaled\n",
    "training_classes = Y_class\n",
    "\n",
    "validation_data = X_attributes_test_scaled\n",
    "validation_classes = Y_class_test\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "# Training the model\n",
    "knn_model.fit(training_attributes, training_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(validation_data)\n",
    "\n",
    "## Metrics\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(validation_classes, Y_predicted)}')\n",
    "\n",
    "print(f\"F1 Score: {f1_score(validation_classes, Y_predicted, average='weighted')}\")\n",
    "\n",
    "# Classification report, which includes precision, recall, f1-score and support\n",
    "print(classification_report(validation_classes, Y_predicted))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(validation_classes, Y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7\n",
    "- `Neighbors`: 19\n",
    "- `Weights`: uniform\n",
    "- `Data normalization`: No\n",
    "- `All features`: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6830985915492958\n",
      "F1 Score: 0.8117154811715481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.68      0.81       284\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.68       284\n",
      "   macro avg       0.50      0.34      0.41       284\n",
      "weighted avg       1.00      0.68      0.81       284\n",
      "\n",
      "[[194  90]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "k = 19\n",
    "weights = 'uniform'\n",
    "\n",
    "training_attributes = X_attributes_less_attributes\n",
    "training_classes = Y_class\n",
    "\n",
    "validation_data = X_attributes_test_less_attributes\n",
    "validation_classes = Y_class_test\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "# Training the model\n",
    "knn_model.fit(training_attributes, training_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(validation_data)\n",
    "\n",
    "## Metrics\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(validation_classes, Y_predicted)}')\n",
    "\n",
    "print(f\"F1 Score: {f1_score(validation_classes, Y_predicted, average='weighted')}\")\n",
    "\n",
    "# Classification report, which includes precision, recall, f1-score and support\n",
    "print(classification_report(validation_classes, Y_predicted))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(validation_classes, Y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 8\n",
    "- `Neighbors`: 19\n",
    "- `Weights`: uniform\n",
    "- `Data normalization`: Yes\n",
    "- `All features`: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6584507042253521\n",
      "F1 Score: 0.7940552016985138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.79       284\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.66       284\n",
      "   macro avg       0.50      0.33      0.40       284\n",
      "weighted avg       1.00      0.66      0.79       284\n",
      "\n",
      "[[187  97]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "k = 19\n",
    "weights = 'uniform'\n",
    "\n",
    "training_attributes = X_attributes_scaled_less_attributes\n",
    "training_classes = Y_class\n",
    "\n",
    "validation_data = X_attributes_test_scaled_less_attributes\n",
    "validation_classes = Y_class_test\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "# Training the model\n",
    "knn_model.fit(training_attributes, training_classes)\n",
    "\n",
    "# Validating the model\n",
    "Y_predicted = knn_model.predict(validation_data)\n",
    "\n",
    "## Metrics\n",
    "# Calculating the accuracy of the model\n",
    "print(f'Accuracy: {accuracy_score(validation_classes, Y_predicted)}')\n",
    "\n",
    "print(f\"F1 Score: {f1_score(validation_classes, Y_predicted, average='weighted')}\")\n",
    "\n",
    "# Classification report, which includes precision, recall, f1-score and support\n",
    "print(classification_report(validation_classes, Y_predicted))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(validation_classes, Y_predicted))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
